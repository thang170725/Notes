-```python
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from backend.app.core.__init__ import Key
import os

# Cache Ä‘á»ƒ cháº¡y nhanh hÆ¡n láº§n sau
CACHE_PATH = "./cache/faiss_index"

# Load documents
print("ğŸ“‚ Loading documents...")
loader = DirectoryLoader(
    "./backend/app/core", 
    glob="*.py", 
    loader_cls=TextLoader,
    loader_kwargs={"encoding": "utf-8"}, 
    exclude=["**/__init__.py"]
)
documents = loader.load()
print(f"âœ… Loaded {len(documents)} files")

# Split vá»›i chunk lá»›n hÆ¡n Ä‘á»ƒ giá»¯ context
splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,  # TÄƒng Ä‘á»ƒ giá»¯ nguyÃªn file nhá»
    chunk_overlap=300,
    separators=["\n\nclass ", "\n\ndef ", "\n\n", "\n"]
)
docs = splitter.split_documents(documents)
print(f"âœ… Split into {len(docs)} chunks\n")

# Vector store vá»›i cache
if os.path.exists(CACHE_PATH):
    print("ğŸ“¦ Loading from cache...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    db = FAISS.load_local(CACHE_PATH, embeddings, allow_dangerous_deserialization=True)
else:
    print("ğŸ”¨ Creating vector store...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    db = FAISS.from_documents(docs, embeddings)
    os.makedirs(os.path.dirname(CACHE_PATH), exist_ok=True)
    db.save_local(CACHE_PATH)
    print("ğŸ’¾ Cached for next run")
print("âœ… Vector store ready\n")

# Setup LLM
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0,
    api_key=Key().get_openai_key(),
    max_tokens=2000
)

# Improved prompt
template = """Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch code Python vÃ  báº£o máº­t.

PhÃ¢n tÃ­ch CODE THá»°C Táº¾ dÆ°á»›i Ä‘Ã¢y:

{context}

CÃ¢u há»i: {question}

Tráº£ lá»i CHI TIáº¾T dá»±a trÃªn code thá»±c táº¿:

1. **Chá»©c nÄƒng**: File lÃ m gÃ¬? CÃ³ nhá»¯ng class/function nÃ o?
2. **Chi tiáº¿t ká»¹ thuáº­t**: Import gÃ¬? Logic ra sao?
3. **Váº¥n Ä‘á» báº£o máº­t**: 
   - Input validation cÃ³ Ä‘á»§ khÃ´ng?
   - CÃ³ hardcoded secrets khÃ´ng?
   - CÃ³ lá»— há»•ng nÃ o khÃ´ng? (injection, XSS, etc.)
4. **Äá» xuáº¥t**: Cáº£i thiá»‡n code vÃ  báº£o máº­t

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, Cá»¤ THá»‚ vÃ  dá»±a trÃªn CODE:"""

prompt = ChatPromptTemplate.from_template(template)

# Format context vá»›i filename
def format_docs(docs):
    result = []
    for doc in docs:
        filename = os.path.basename(doc.metadata['source'])
        result.append(f"=== FILE: {filename} ===\n{doc.page_content}")
    return "\n\n".join(result)

# RAG chain
retriever = db.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}  # Láº¥y nhiá»u hÆ¡n Ä‘á»ƒ Ä‘áº£m báº£o cÃ³ test.py
)

chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt | llm | StrOutputParser()
)

# Run analysis
print("ğŸ” Analyzing...\n")
query = "File test.py Ä‘ang lÃ m gÃ¬? CÃ³ váº¥n Ä‘á» báº£o máº­t gÃ¬ khÃ´ng?"

# Debug: Show retrieved docs
print("ğŸ“š Retrieved documents:")
retrieved = retriever.invoke(query)
for i, doc in enumerate(retrieved, 1):
    filename = os.path.basename(doc.metadata['source'])
    print(f"   {i}. {filename} ({len(doc.page_content)} chars)")

print("\n" + "="*70)
print("ğŸ’¡ PHÃ‚N TÃCH CHI TIáº¾T")
print("="*70 + "\n")

result = chain.invoke(query)
print(result)

print("\n" + "="*70)
print("âœ… HoÃ n táº¥t!")
print("="*70)
```